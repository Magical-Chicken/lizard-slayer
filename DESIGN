Goal:
    - distributed platform for concurrent data processing
    - support for processing on CPUs and CUDA enabled GPUs
    - automatic data management, no manual memory management or data transfer
    - simple api, should only need to define algorithm and basic params
    - code should be independant of hardware
Expectations for End Result:
    - functional implementation of the concurrent data processing platform or a
      subset thereof sufficient for running a demonstration program
    - demonstration program  program or programs and several datasets of
      varying size to evaluate performance and scalability
    - demonstration programs may include parallelizable algorithms previously
      studied in this class such as kmeans
    - data from performance testing, graphs, and analysis of data
    - paper focusing on the concurrency aspects of the project
    - paper focusing on the distributed aspects of the project
    - presentation
Hardware Requirements:
    - group of machines running linux
    - must support concurrent execution, preferrably with many threads
    - some must have CUDA enabled GPUs
    - machines must be connected within the same network
    - GDC lab machines should work, including the GPU equipped eldar machines
Software Overview:
    - api
        - as simple as possible
        - data structure specification
        - data source config, i.e. from file, or from sql or output of func
        - algorithm specification, preferrably in a high level language
        - optional configuration for runtime systems
        - timing and performance monitoring options
        - output representation of planned workflow
        - output graph of hardware network
    - hardware discovery system
        - runs as a client is started
        - detects CPU threads available
        - detects if CUDA compatible GPU is present
        - control server builds database of available hardware
        - possibly test and profile network performance between nodes
    - task planning system
        - partition workload into workunits
        - partition allocs for working data
        - partition input data
        - optimize placement of workloads between nodes to make best use of
          hardware capabilities
    - distributed control system
        - central control server
        - client software connects to server and waits for workunits
        - must manage data transfer efficiently over the network
        - possibly add support for fault tolerance
    - logic translation system
        - mechanism to translate algorithm into CUDA
        - system to compile CUDA code to the highest compute level supported by
          all hardware, and to distribute compiled binaries to worker nodes
        - system to compile algorithm for CPU
Design Decisions (not finalized) :
    - support only CUDA to run code on GPUs for now, as this will simplify
      development significantly
    - implement most systems in python, because we know it well and it will
      allow fast development and testing
Timeline:
    - Week 1
        - plan project, write project proposal
        - set up project
        - get base worker client and server running
        - get hardware discovery and output graph working
    - Week 2
        - get logic translation system working
        - get clients accepting workunits from server
        - by end of week 2 should be possible for client to run code sent out
          by server
    - Week 3
        - build and document api
        - get task planning system working
        - build out runtime interface
    - Week 4
        - bug fixing
        - testing
        - develop demo program
        - measure and review performance
        - write papers
        - plan presentation
